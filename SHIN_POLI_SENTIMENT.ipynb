{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSPB 3022 Classification Project - Political Text Analysis/Predictions Using Supervised Learning Methods\n",
    "## Theo Shin\n",
    "\n",
    "***\n",
    "\n",
    "<figure>\n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\" title = \"FINAL PROJECT\">\n",
    "</figure>\n",
    "<img src=\"trump.jpg\" alt=\"Drawing\" WIDTH=600 />\n",
    "\n",
    "  \n",
    "## Introduction\n",
    "This dataset contains the classification of partisan bias, audience, and goals based on politicians' social media. Total rows total ~5000 and the objective is to clean, organize, manipulate data contents to derive meaningful conclusions based on hypotheses and analysis. For the final project, I have chosen sentiment analysis classifying political bias provided texts (in this case tweets). That is, to assign categories to the collection of data (political messaging) in order to aid in more accurate predictions and analysis. The idea of crowdsourcing draws on \"wisdom of the crowd\" arguments where I used the CrowdFlower/Kaggle database to access the data set to be used for this project, titled, \"Political Social Media Posts\" (https://www.kaggle.com/crowdflower/political-social-media-posts/data).\n",
    "    \n",
    "I chose this topic in particular, not because I'm fascinated with politics, but for the harnessing power of social media and was curious to see how natural language processing (NLP) could computationally identify and categorize opinions expressed in the political messages to determine potential biases. \n",
    "\n",
    "Now that a multivariate analysis problem of interest has been identified, the outline of the project shall continue as follows:\n",
    "1. Selecting Data Sources\n",
    "2. Preprocess data; Cleaning and transforming data, as needed\n",
    "3. Perform Exploratory Data Analysis (EDA)\n",
    "4. Perform Classification\n",
    "    \n",
    "\n",
    "*** References Disclaimer ***    \n",
    "Websites used convert text into meaningful encoding vectors using some corpus (feature vectors may vary widely depending on the proximity of the corpus to the original problem), along with possibly tools to enhance visual flare. \n",
    "\n",
    "    \n",
    "    If you referenced any web sites or solutions not of your own creation, list those references here:\n",
    "* Word Counter: https://data-flair.training/blogs/python-counter/\n",
    "* TextBlob Class: https://textblob.readthedocs.io/en/dev/api_reference.html\n",
    "* Classifiers: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html; https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "* TfidfTransformer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "* CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "* Naive Bayes Classifier for Text Analysis: https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67\n",
    "* Pipeline: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "* Sentiment Analysis Overview: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5635074/\n",
    "* Support Vector Machines: https://scikit-learn.org/stable/modules/svm.html\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations:\n",
    "***\n",
    "#### textblob and wordcloud (pip install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.classifiers import DecisionTreeClassifier\n",
    "import scipy.stats\n",
    "import sklearn.linear_model\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub: https://github.com/tshin23/Political-Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Sentiment Analysis](#sentiment)\n",
    "    * [Political Texts](#poli)\n",
    "    * [Data Source](#Crowdcoding)\n",
    "    * [Dataset Description](#desc)\n",
    "    * [Considerations](#consider)\n",
    "3. [Exploratory Data Analysis](#EDA)\n",
    "    * [Expected Outcomes](#expect)\n",
    "    * [Hypotheses](#hypothesis)\n",
    "    * [Quick Look at Data](#quick)\n",
    "    * [Word Count](#word)\n",
    "    * [Character Count](#char)\n",
    "    * [# Hashtag Count](#hash)\n",
    "    * [@ Directed Tweets](#@)\n",
    "    * [Average Word Length](#length)\n",
    "4. [Pre-Processing](#process) \n",
    "    * [Transform into Lower Case](#lower)\n",
    "    * [Remove Punctuation](#punc)\n",
    "    * [Stop Words](#stop)\n",
    "    * [Word Frequency](#freq)\n",
    "    * [Outlying Words](#rare)\n",
    "    * [Spelling Bee](#spell)\n",
    "5. [VISUALS for Text Analysis](#visual) \n",
    "    * [Bar Graph for Related Words](#bar)\n",
    "    * [Word Cloud](#wordcloud)\n",
    "    * [Spelling Bee](#spell)\n",
    "6. [Classification](#class) \n",
    "    * [Multinomial Naive Bayes](#mnb)\n",
    "    * [Support Vector Machines](#svm)\n",
    "7. [Predictive Analysis](#pred)\n",
    "    * [Sentiment Text Analysis](#sentimentanalysis)\n",
    "8. [Conclusion](#conc)\n",
    "    * [Reference: Stop Words](#stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentiment'></a>\n",
    "### Sentiment Analysis\n",
    "Sentiment analysis is important in the studies of news values, public opinion, negative campaigning or political polarization and the expansion of digital textual data and efficient progress in automated text analysis provides vast opportunities for innovated social sciences [research] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5635074/). In general, a lack of tools and procedures for producing or collecting sentiment ratings of acceptable quality for large-scale data analyses currently limits the ease of use with this data mining technique. Typically with emphasis on large scale data, research projects can quickly overwhelm when faced with restrictions of time, money, limited trained coders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='poli'></a>\n",
    "### Measuring Sentiment in Political Texts\n",
    "\n",
    "Analyzing the polarity of texts has a long tradition in social sciences. A prominent example is media negativity, capturing the over-selection of negative over positive news, the tonality of media stories, and the degree of conflict or confrontation in news. With sentiment analysis, a large dataset can be classified with variying polarity, valence or tone and classified as simply positive, negative, or neutral. For the purposes of this project, supervised learning automated approaches will be employed to \"learn\" the sentiment of political messages (Note - the instructions governing the dataset will be kept at a beginner due to time constraints and limits on the tools available to me). Latter analyses will introduce supervised learning techniques from course discussions and set up the basics of supervised machine learning in the realm of text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Crowdcoding'></a>\n",
    "### Data Source\n",
    "\n",
    "As noted in the introduction, the idea of crowdsourcing draws on \"wisdom of the crowd\" arguments where I used the CrowdFlower/Kaggle database to access the data set to be used for this project, titled, \"Political Social Media Posts\" (https://www.kaggle.com/crowdflower/political-social-media-posts/data). For project consideration, we will later delve deeper into selecting relevant words from the political messages to try and accurately predict meaning and predictions. \n",
    "\n",
    "This dataset contains the results from contributors analyzing thousands of social media messages from U.S. Senators and other American politicians to classify its contents. Messages were broken down into audience (national or the tweeterâ€™s constituency), bias (neutral/bipartisan, or biased/partisan), and finally tagged as the actual substance of the message itself (options ranged from informational, announcement of a media appearance, an attack on another candidate, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='desc'></a>\n",
    "### Full Data Description: [political_social_media.csv]\n",
    "\n",
    "1. __unit_id__ : A unique id for the message\n",
    "2. __golden__: always FALSE; (classifier for whether the message meets Crowdflower's gold standard) \n",
    "3. __unit_state__: always \"finalized\"\n",
    "4. __trusted_judgments__: the number of trusted human judgments that were entered for this message; an integer between 1 and 3\n",
    "5. __last_judgment_at__: when the final judgment was collected\n",
    "6. __audience__: one of national or constituency\n",
    "7. __audience:confidence__: a measure of confidence in the audience judgment; a float between 0.5 and 1\n",
    "8. __bias__: one of neutral of partisan\n",
    "9. __bias:confidence__: a measure of confidence in the bias judgment; a float between 0.5 and 1\n",
    "10. __message__: the aim of the message. one of: -- attack: the message attacks another politician \n",
    "    * (0) constituency: the message discusses the politician's constituency \n",
    "    * (1) information: an informational message about news in government or the wider U.S.  \n",
    "    * (2) media: a message about interaction with the media \n",
    "    * (3) mobilization: a message intended to mobilize supporters \n",
    "    * (4) other: a catch-all category for messages that don't fit into the other \n",
    "    * (5) personal: a personal message, usually expressing sympathy, support or condolences, or other personal opinions \n",
    "    * (6) policy: a message about political policy \n",
    "    * (7) support: a message of political support\n",
    "12. __message:confidence__: a measure of confidence in the message judgment; a float between 0.5 and 1\n",
    "13. __orig_golden__: always empty; presumably whether some portion of the message was in the gold standard\n",
    "14. __audience_gold__: always empty; presumably whether the audience response was in the gold standard\n",
    "15. __bias_gold__: always empty; presumably whether the bias response was in the gold standard\n",
    "16. __bioid__: a unique id for the politician\n",
    "17. __embed__:HTML code to embed this message\n",
    "18. __id__: unique id for the message WITHIN whichever social media site it was pulled from\n",
    "19. __label__: a string of the form \"From: firstname lastname (position from state)\"\n",
    "20. __message_gold__: always blank; presumably whether the message response was in the gold standard\n",
    "21. __source__: where the message was posted; one of \"facebook\" or \"twitter\"\n",
    "17. __text__: the text of the message\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='consider'></a>\n",
    "### Dataset Considerations\n",
    "* Description from kaggle: This dataset, from Crowdflower's Data For Everyone Library, provides text of 5000 messages from politicians' social media accounts, along with human judgments about the purpose, partisanship, and audience of the messages. Because contents of this dataset are subject to human judgements, biases of the message components, it will be interesting to observe how much of the data is subjected to human persuasion (especially confidence level measurements). \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='EDA'></a>\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='expect'></a>\n",
    "### Expected Outcomes\n",
    "\n",
    "From this point, I will: \n",
    "1. Explore, clean, and modify the political message dataset to determine meaningful relationships between certain words and message types as indictaed by qualified human assessors from varying social media posts.\n",
    "\n",
    "2. Determine how certain words in tweets/posts are correlated with political messages using heatmap/wordcloud.\n",
    "\n",
    "3. Tranform data and perform linear, logistic regression, or other classification methods -- Decision tree classifiers: applying Naive Bayes or Support Vector Machine based algorithms for supervised machine-learning assessment.\n",
    "\n",
    "4. Determine which classification based method is the stronger predictor for political bias/alignments between Multinomial Naive Bayes (MNB) text classification and Support Vector Machine (SVM) machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hypothesis'></a>\n",
    "### Hypotheses\n",
    "\n",
    "1. The SVM classifier will out-predict and provide more accurate results than MNB\n",
    "\n",
    "2. Anticipation of certain words being highly correlated with certain messages: \n",
    " * veterans = policy; support\n",
    " * bill = policy\n",
    " * In case you missed it (ICYMI) = media\n",
    " * abortion = policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='quick'></a>\n",
    "### Quick Look at Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the first five rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>audience</th>\n",
       "      <th>audience:confidence</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias:confidence</th>\n",
       "      <th>message</th>\n",
       "      <th>...</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>audience_gold</th>\n",
       "      <th>bias_gold</th>\n",
       "      <th>bioid</th>\n",
       "      <th>embed</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>message_gold</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>766192484</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/2015 21:17</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R000596</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.83E+17</td>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>766192485</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/2015 21:20</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M000355</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.11E+17</td>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>766192486</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/2015 21:14</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>support</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S001180</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>3.39E+17</td>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766192487</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/2015 21:08</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C000880</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>2.99E+17</td>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>766192488</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>1</td>\n",
       "      <td>8/4/2015 21:26</td>\n",
       "      <td>national</td>\n",
       "      <td>1.0</td>\n",
       "      <td>partisan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>policy</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U000038</td>\n",
       "      <td>&lt;blockquote class=\"twitter-tweet\" width=\"450\"&gt;...</td>\n",
       "      <td>4.08E+17</td>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  766192484    False   finalized                   1    8/4/2015 21:17   \n",
       "1  766192485    False   finalized                   1    8/4/2015 21:20   \n",
       "2  766192486    False   finalized                   1    8/4/2015 21:14   \n",
       "3  766192487    False   finalized                   1    8/4/2015 21:08   \n",
       "4  766192488    False   finalized                   1    8/4/2015 21:26   \n",
       "\n",
       "   audience  audience:confidence      bias  bias:confidence  message  \\\n",
       "0  national                  1.0  partisan              1.0   policy   \n",
       "1  national                  1.0  partisan              1.0   attack   \n",
       "2  national                  1.0   neutral              1.0  support   \n",
       "3  national                  1.0   neutral              1.0   policy   \n",
       "4  national                  1.0  partisan              1.0   policy   \n",
       "\n",
       "                         ...                          orig__golden  \\\n",
       "0                        ...                                   NaN   \n",
       "1                        ...                                   NaN   \n",
       "2                        ...                                   NaN   \n",
       "3                        ...                                   NaN   \n",
       "4                        ...                                   NaN   \n",
       "\n",
       "   audience_gold  bias_gold    bioid  \\\n",
       "0            NaN        NaN  R000596   \n",
       "1            NaN        NaN  M000355   \n",
       "2            NaN        NaN  S001180   \n",
       "3            NaN        NaN  C000880   \n",
       "4            NaN        NaN  U000038   \n",
       "\n",
       "                                               embed        id  \\\n",
       "0  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.83E+17   \n",
       "1  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.11E+17   \n",
       "2  <blockquote class=\"twitter-tweet\" width=\"450\">...  3.39E+17   \n",
       "3  <blockquote class=\"twitter-tweet\" width=\"450\">...  2.99E+17   \n",
       "4  <blockquote class=\"twitter-tweet\" width=\"450\">...  4.08E+17   \n",
       "\n",
       "                                              label message_gold   source  \\\n",
       "0    From: Trey Radel (Representative from Florida)          NaN  twitter   \n",
       "1     From: Mitch McConnell (Senator from Kentucky)          NaN  twitter   \n",
       "2  From: Kurt Schrader (Representative from Oregon)          NaN  twitter   \n",
       "3          From: Michael Crapo (Senator from Idaho)          NaN  twitter   \n",
       "4          From: Mark Udall (Senator from Colorado)          NaN  twitter   \n",
       "\n",
       "                                                text  \n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2  Please join me today in remembering our fallen...  \n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4  .@amazon delivery #drones show need to update ...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext = pd.read_csv('political_social_media.csv')\n",
    "politext.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 observations and 21 features in this dataset. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} observations and {} features in this dataset. \\n\".format(politext.shape[0],politext.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_unit_id   \t# of unique values: 5000,   \tdtype: int64\n",
      "_golden   \t# of unique values: 1,   \tdtype: bool\n",
      "_unit_state   \t# of unique values: 1,   \tdtype: object\n",
      "_trusted_judgments   \t# of unique values: 3,   \tdtype: int64\n",
      "_last_judgment_at   \t# of unique values: 238,   \tdtype: object\n",
      "audience   \t# of unique values: 2,   \tdtype: object\n",
      "audience:confidence   \t# of unique values: 31,   \tdtype: float64\n",
      "bias   \t# of unique values: 2,   \tdtype: object\n",
      "bias:confidence   \t# of unique values: 35,   \tdtype: float64\n",
      "message   \t# of unique values: 9,   \tdtype: object\n",
      "message:confidence   \t# of unique values: 23,   \tdtype: float64\n",
      "orig__golden   \t# of unique values: 5000,   \tdtype: float64\n",
      "audience_gold   \t# of unique values: 5000,   \tdtype: float64\n",
      "bias_gold   \t# of unique values: 5000,   \tdtype: float64\n",
      "bioid   \t# of unique values: 505,   \tdtype: object\n",
      "embed   \t# of unique values: 5000,   \tdtype: object\n",
      "id   \t# of unique values: 2763,   \tdtype: object\n",
      "label   \t# of unique values: 505,   \tdtype: object\n",
      "message_gold   \t# of unique values: 5000,   \tdtype: float64\n",
      "source   \t# of unique values: 2,   \tdtype: object\n",
      "text   \t# of unique values: 5000,   \tdtype: object\n"
     ]
    }
   ],
   "source": [
    "#Looking deeper into the type of values in each column\n",
    "for column in politext.columns:\n",
    "    print(str.format(\"%s   \\t# of unique values: %s,   \\tdtype: %s\" % (column, len(np.unique(politext[column].values)), politext[column].dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word'></a>\n",
    "### Let's count the number of words in each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...          11\n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...          12\n",
       "2  Please join me today in remembering our fallen...          22\n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...          20\n",
       "4  .@amazon delivery #drones show need to update ...          20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext['word_count'] = politext['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "politext[['text','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='char'></a>\n",
    "### How about the number of characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  char_count\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...          93\n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...          85\n",
       "2  Please join me today in remembering our fallen...         136\n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...         124\n",
       "4  .@amazon delivery #drones show need to update ...         143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext['char_count'] = politext['text'].str.len() ## this also includes spaces\n",
    "politext[['text','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hash'></a>\n",
    "### Number of hashtags per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hashtags\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...         3\n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...         1\n",
       "2  Please join me today in remembering our fallen...         0\n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...         1\n",
       "4  .@amazon delivery #drones show need to update ...         4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext['hashtags'] = politext['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "politext[['text','hashtags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='@'></a>\n",
    "### Number of directed tweets (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  at\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...   1\n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...   0\n",
       "2  Please join me today in remembering our fallen...   0\n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...   1\n",
       "4  .@amazon delivery #drones show need to update ...   0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext['at'] = politext['text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
    "politext[['text','at']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='length'></a>\n",
    "### Average word length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "      <td>7.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "      <td>6.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "      <td>5.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "      <td>5.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "      <td>6.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  avg_word\n",
       "0  RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  7.545455\n",
       "1  VIDEO - #Obamacare:  Full of Higher Costs and ...  6.727273\n",
       "2  Please join me today in remembering our fallen...  5.227273\n",
       "3  RT @SenatorLeahy: 1st step toward Senate debat...  5.250000\n",
       "4  .@amazon delivery #drones show need to update ...  6.200000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "politext['avg_word'] = politext['text'].apply(lambda x: avg_word(x))\n",
    "politext[['text','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='process'></a>\n",
    "## Data Pre-Processsing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scope of the hypotheses are focused on message biases from the text, let's only focus on the relevant columns of \"Message\", \"Bias\", \"Label\", and \"Text\" and remove all other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Remove columns to keep only those subject to analysis\n",
    "politext = politext.drop(['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'audience', \\\n",
    "                'audience:confidence', 'bias:confidence', 'message:confidence', 'orig__golden', 'audience_gold', \\\n",
    "               'bias_gold', 'bioid', 'embed', 'id', 'message_gold', 'source'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lower'></a>\n",
    "## Transform tweets into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt @nowthisnews: rep. trey radel (r- #fl) slam...\n",
       "1    video - #obamacare: full of higher costs and b...\n",
       "2    please join me today in remembering our fallen...\n",
       "3    rt @senatorleahy: 1st step toward senate debat...\n",
       "4    .@amazon delivery #drones show need to update ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext['text'] = politext['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "politext['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='punc'></a>\n",
    "## Remove punctuation to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt nowthisnews rep trey radel r fl slams obama...\n",
       "1    video  obamacare full of higher costs and brok...\n",
       "2    please join me today in remembering our fallen...\n",
       "3    rt senatorleahy 1st step toward senate debate ...\n",
       "4    amazon delivery drones show need to update law...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politext['text'] = politext['text'].str.replace('[^\\w\\s]','')\n",
    "politext['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stop'></a>\n",
    "## Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt nowthisnews rep trey radel r fl slams obama...\n",
       "1    video obamacare full higher costs broken promi...\n",
       "2    please join today remembering fallen heroes ho...\n",
       "3    rt senatorleahy 1st step toward senate debate ...\n",
       "4    amazon delivery drones show need update law pr...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "politext['text'] = politext['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "politext['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='freq'></a>\n",
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "today        715\n",
       "us           437\n",
       "house        433\n",
       "amp          409\n",
       "great        395\n",
       "new          361\n",
       "bill         323\n",
       "act          291\n",
       "congress     289\n",
       "president    284\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(politext['text']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rare'></a>\n",
    "## Outliers: Rarest Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usnews                  1\n",
       "lying                   1\n",
       "dashevsky               1\n",
       "ampdisproportionally    1\n",
       "aew2014                 1\n",
       "wickford                1\n",
       "proceed                 1\n",
       "govchristie             1\n",
       "uaw                     1\n",
       "falllike                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(politext['text']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Remove rarely occuring words (Removal reduces vocabulary clutter so features used later are effective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = list(freq.index)\n",
    "# politext['text'] = politext['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "# politext['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='spell'></a>\n",
    "### Spelling Mistakes/Further Message Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "politext['text'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Bias and Message Columns to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And Convert the bias and message columns to categorical (this will be helpful for classification methods later on)         \n",
    "politext.bias = pd.Categorical(politext.bias)\n",
    "politext.message = pd.Categorical(politext.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a \"VECTOR\" column to create a vector of the TEXT field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also set up a new column labeled \"vector\" for words in text field\n",
    "politext['vector'] = [i for i in politext.text.str.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now look at the first 10 rows (capitalized column headers)\n",
    "politext.bias = pd.Categorical(politext.bias)\n",
    "renameColumns = {'bias' : 'BIAS', 'message' : 'MESSAGE', 'label' : 'LABEL', 'text' : 'TEXT', 'vector' : 'VECTOR'}\n",
    "politext.rename(mapper = renameColumns, axis = 1, inplace=True)\n",
    "politext.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again let's look at the type of values for the remaining columns\n",
    "for column in politext.columns:\n",
    "    print(\"%s       \\t# of unique values: %s,   \\tdtype: %s\" % (column, len(np.unique(politext[column].values)), politext[column].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} observations and {} features in this dataset. \\n\".format(politext.shape[0],politext.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visual'></a>\n",
    "## VISUALS FOR TEXT ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETERMINING 50 MOST/LEAST COMMON WORDS USED IN TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = Counter()\n",
    "politext.VECTOR.apply(count_words.update)\n",
    "print('MOST COMMON WORD (COUNT):\\n', count_words.most_common(50))\n",
    "print('\\n\\n')\n",
    "print('LEAST COMMON WORD (COUNT):\\n', count_words.most_common()[-250:-200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've already taken the liberty to clean up articles and common words (using stopwords) we're left with words that are relevantly least used along with words with incorrect spacing, continued misspellings, and hyperlinks. In this case, the least common words used may not have the intended effect for analysis. \n",
    "\n",
    "For now, any such discrepancies will be noted and possibly mitigated depending on its effect on the data. In general, as noted during the initial proposal assessment, due to the sentiment analysis classifying political bias given text, categorical classification, correlation coefficient and further regression analysis need not to apply to describing the data. Instead, meaningful encoding vectors and classifiers (concepts discussed in the later weeks) will be used to make further sense of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bar'></a>\n",
    "### Bar Graph Representing Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = politext.MESSAGE.unique()\n",
    "y = politext.MESSAGE.value_counts()\n",
    "plt.bar(x, y);\n",
    "plt.xticks(rotation=30);\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Messages:\\n\",politext.MESSAGE.describe())\n",
    "print(\"\\nBiased word choices:\\n\",politext.BIAS.describe())\n",
    "\n",
    "\n",
    "print(\"\\nMessages containing the word 'proud':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"proud\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'love':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"love\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'hate':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"hate\")].describe())\n",
    "print(\"\\nMessages containing the word 'ICYMI' (In Case You Missed It):\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"icymi\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'Obama':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"obama\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'bill':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"bill\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'veterans':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"veterans\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'immigrants':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"immigrants\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'abortion':\\n\",politext.MESSAGE[politext.TEXT.str.contains(\"abortion\")].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph shows a heavy positive skew towards 'policy', 'attack', 'support' type messages. Though these results were somewhat expected, it may be a clear indicator of favoritism towards certain classes of words which influence the 2nd hypothesis (How certain words in tweets/posts are correlated with political messages and their implications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = politext['LABEL'].head(n=15000)\n",
    "states = data.groupby(politext['LABEL']).count()\n",
    "states.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wordcloud'></a>\n",
    "## WORDCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What it is..\n",
    "#?WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual of WORDCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a visual of the TEXT column\n",
    "ptext = politext.TEXT[0]\n",
    "\n",
    "# Create and generate a world cloud image:\n",
    "wordcloud = WordCloud().generate(ptext)\n",
    "\n",
    "# lower max_font_size, change maximum number of words:\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100).generate(ptext)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delving deeper into word analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptext = \" \".join(review for review in politext.TEXT)\n",
    "print (\"There are {} words in the combination of all tweets.\".format(len(ptext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(politext)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')\n",
    "# Create stopword list:\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords, width=1600, height=800, background_color = 'white').generate(ptext)\n",
    "plt.figure(figsize=(20,10), facecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('wordcloud.png', facecolor='k', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPOTHESIS #2: Anticipation of certain words being highly correlated with certain messages: \n",
    " * veterans = policy; support\n",
    " * bill = policy\n",
    " * In case you missed it (ICYMI) = policy (Original prediction was media)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, many of the topics are correlated with policy and expressions such as \"proud\" and \"love\" are equated with personal-type messages, though \"hate\" came back as policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='class'></a>\n",
    "## CLASSIFICATION \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Logistic Regression and Transforming the Text Data\n",
    "\n",
    "Now for the fun part. For the data/text, the string values will be tokenized based upon the frequency of its occurrence in the corpus of the text. Based on the columns resulting from earlier data cleansing, the categorical values will be encoded into integers using a label encoder. \n",
    "\n",
    "Following tokenization (process of replacing sensitive data with unique identification symbols that retain all the essential information about the data without compromising its security), the resulting data will be split into training and testing data. I'll begin by implementing a standard test size of 30% across each of the models to ascertain semi-optimal increase in the model's accuracy. Note - in cases of smaller test sizes, certain test cases may reveal higher accuracies due to coincidence.\n",
    "\n",
    "In order for our classifiers to process the string data of our textual fields, they need to be tokenized to numerical vectors. This process is different for our target and main data. For the target, because it is a categorical variable, we need to encode those categories into integers using a Label Encoder. For our data, because it is a set of discrete strings, we need to tokenize them on the basis of the frequency of their occurance in the corpus of text.\n",
    "\n",
    "After transforming our target and data, we need to split our training and testing data. We will use a standard test size of 33% of the dataset across our models, as this value seems to be semi-optimal for increasing our model's accuracy. With smaller test sizes, you may get higher point-accuracies for certain runs, purely due to coincidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(politext,test_size = 0.1)\n",
    "# Removing \" \" for later use\n",
    "train = train[train.VECTOR != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder definition\n",
    "label_encode = sklearn.preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = social media text; Target = assigned message\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "X, y = politext.TEXT, politext.MESSAGE\n",
    "label_encode.fit(y)\n",
    "y = label_encode.transform(y)\n",
    "\n",
    "#tokenize the frequency of text occurrence and categorical values (target) to pass as numerals\n",
    "vector_count = CountVectorizer()\n",
    "X_counts = vector_count.fit_transform(X)\n",
    "tfidf_X = TfidfTransformer() #transform count matrix to normalized tf(term frequency)-idf representation\n",
    "token_X = tfidf_X.fit_transform(X_counts)\n",
    "X = token_X\n",
    "\n",
    "#Splitting of data for training/testing\n",
    "train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "#Label Encode messages\n",
    "print(\"Categorical Targets:\", np.unique(y))\n",
    "print(\"Message Types:\", np.unique(label_encode.inverse_transform(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='mnb'></a>\n",
    "# Applying Multinomial Naive Bayes (MNB) Classification for Text Analysis\n",
    "\n",
    "\n",
    "From medium.com and Wikipedia, Naive Bayes is a family of algorithms based on applying Bayes theorem with a strong(naive) assumption, that every feature is independent of the others, in order to predict the category of a given sample. Naive Bayes will be used for text categorization, in this case, evaluating the text as belonging to one category or another with word frequencies as features. I will be classifying the Targets/Message Types from a model predictor in a supervised learning setting.\n",
    "\n",
    "* For more information regarding the use of MNB for encoding vectors, please see: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNB initialization\n",
    "MNB = MultinomialNB()\n",
    "#MNB fit\n",
    "classifier = MNB.fit(train_X, train_y)\n",
    "#MNB predictor\n",
    "predicted_classes = classifier.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "print(\"MNB Classification Metrics: \\n\\n\", metrics.classification_report(test_y, predicted_classes, target_names = np.unique(label_encode.inverse_transform(y))))\n",
    "print(\"MNB Confusion Matrix: \\n\\n\", metrics.confusion_matrix(test_y, predicted_classes))\n",
    "print(\"Accuracy of MNB prediction: \", np.mean(predicted_classes == test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: Overall, how often is the classifier correct?\n",
    "\n",
    "Precision: When it predicts yes, how often is it correct?\n",
    "\n",
    "F Score: This is a weighted average of the true positive rate (recall) and precision.\n",
    "\n",
    "\n",
    "Based on the MNB metrics and confusion matrix, it seems to be the case of over-prediction. With 1500 unique words in the dataset this may be the result of higher word frequencies, which was noted earlier for the histogram of words in each class. The confusion matrix shows a heavy right-sided skew which could mean the MNB is only calculating particular messages. Let's compare the results with the SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='svm'></a>\n",
    "## Meaningful Vector Encoding (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline Creation\n",
    "X, y = politext.TEXT, politext.MESSAGE\n",
    "train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(X, y, test_size=0.33)\n",
    "poli_pipe = Pipeline([('vector_count', CountVectorizer()), ('tfidf_X', TfidfTransformer()), ('pipeline', SGDClassifier(loss='squared_loss', penalty='elasticnet', alpha=1e-3, \n",
    "                                                                                                       random_state=42, max_iter=10, tol=None, class_weight='balanced')),]);\n",
    "#Classification Fit\n",
    "poli_class = poli_pipe.fit(train_X, train_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction for test messages; Classification metrics/confusion matrix for SVM\n",
    "prediction = poli_pipe.predict(test_X)\n",
    "print(\"Accuracy of MNB predictor: \", np.mean(prediction == test_y))\n",
    "warnings.simplefilter('ignore')\n",
    "print(\"\\n\\nSVM Classification Metrics: \\n\\n\", metrics.classification_report(test_y, prediction))\n",
    "print(\"\\n\\nSVM Confusion Matrix: \\n\\n\", metrics.confusion_matrix(test_y, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the MNB and SVM classifiers, the accuracies came back with better than chance results (1/9 = 0.1111). However, accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). After numerous testing, the MNB classifier achieved accuracy of around ~ 0.33 and the SVM at ~ 0.318. As noted earlier, although the MNB achieves a certain level of accuracy, it struggles with over prediction and therefore those tendencies are reflected in the results. At the end of the day, if asked to choose between the two model predictors, the SVM remains quite accurate while still considering a wider range of message types. When fed larger textual data, the SVM would learn and continue to improve as sample size increases. Both models reflected positive results and could be a case-by-case basis on which model is better suited for different situations. Let's now examine a function to get a better idea of how the models interpreted which words for each message type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Accuracy for Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 50\n",
    "accuracy = np.empty(runs)\n",
    "for i in range(len(accuracy)):\n",
    "    train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(X, y, test_size = 0.3) #same percentage as earlier\n",
    "    poli_pipe.fit(train_X, train_y);\n",
    "    predict = poli_pipe.predict(test_X)\n",
    "    accuracy[i] = np.mean(predict == test_y)\n",
    "    \n",
    "print('The accuracy (average) for {} runs is: {}'.format(runs, np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For 50 runs, the average accuracy hovers around 0.312.\n",
    "For the model to be better than chance, we have to achieve over 0.11 probability of making a correct prediction. Essentially being able to guess correctly 1 our of every 9 times. \n",
    "\n",
    "For the Multinomial NB classifier, accuracy was pretty high at near 40% levels. This is certainly better than chance, though fairly inconsistent and as noted accuracy isn't the most reliable metric. One glaring issue was its tendency to predict ALL tweets were exclusively only two message types. Because the majority of tweets fall into just a few message categories, the classifer over-predicts for those categories and naturally, is more likely report higher accuracy.\n",
    "\n",
    "Given that our average accuracy over many runs for the SVM classifier is ~ 0.32, the model still performs fairly well. With a larger data set and more diverse set of labelled examples (where each message type has >5000 samples), our SVM model should be able to continue to improve.\n",
    "\n",
    "While our accuracy is not strictly greater than the accuracy achieved with Multinomial NB, we can still conclude that Hypothesis #1 is correct and that the SVM is a better predictor than Multinomial Naive Bayes. The reasoning lies in the confusion matrix. The SVM very clearly considers each message type as a possibility which yields more applicable and usable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentimentanalysis'></a>\n",
    "## A Little More Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politext['TEXT'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a tuple representing polarity and subjectivity of each tweet. Values closer to 1 mean a positive sentiment and values closer to -1 means negative sentiment. We can being using this feature to build our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politext['sentiment'] = politext['TEXT'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "politext[['TEXT','sentiment']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pred'></a>\n",
    "## Predictive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pred_analysis(vector_count, pipeline, class_labels):\n",
    "    names = vector_count.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top = np.argsort(pipeline.coef_[i])[-20:]\n",
    "        print(\"\\n%s: %s\\n\" % (class_label, \" \".join(names[j] for j in top)))\n",
    "        \n",
    "print(\"Featured Words Interpreted for Message Types:\\n\")\n",
    "pred_analysis(poli_pipe.get_params()['vector_count'], poli_pipe.get_params()['pipeline'], np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inform_feature function prints the most informative words the models used to predict the message type for a given tweet. Glancing over the printed list, which is not static but rather changes each train/test split, many of these features make intuitive sense. For instance, in 'support' the expressions 'congratulations,' 'honoring,' and 'thank' are used. For 'attack' you see 'enforcing' and 'obamacare', in 'media' you see 'cspan', 'live', 'watch', 'interview', and 'tune'. You also see some clear examples of overfitting. If there is a single tweet with a unique word mapping to one message type, it is likely that the model will use it and place heavy emphasis  on the unique feature. Certain words do map strongly towards specific message types and therefore, some tweaking would be involved (beyond the scope of this project, perhaps). The SVM model predicts fairly well and has an additional capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities (Beyond Scope of Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at tweets from four Congressional representatives from Colorado, we can do a hypothetical test where potentially the capabilities of the predictor could be used, for example, to be linked with a live Twitter media feed, continuously consuming live content, and classfying the tweets. In turn, with more complex implemenation, the predictors could do more than just classify tweets and have a wide realm of use in various industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweets = [\"ICYMI: Be sure to watch this @FOX21News clip on my recent trip to Afghanistan to visit our Colorado-based troops. I'm incredibly proud of the work these men and women do everyday.\", \n",
    "                 \"My job is to represent Colorado in the United States Senate and that means working across party lines to get things done for the state. I'm proud to be ranked as the 8th most bipartisan Senator. More info here:\",\n",
    "                \"Having only a few hours to read and digest huge bills is an absurd way to run a government. Youâ€™d be upset if your teacher assigned War and Peace in the morning and tested you on it later that afternoon. #READIT\", \n",
    "                 \"Please keep in your prayers all of those impacted by the fires burning across our district right now and those who are working to combat these blazes.\",\n",
    "                \"Ending TPS for Hondurans forces tens of thousands of law-abiding individualsâ€”many of whom are parents to children who are U.S. citizensâ€”back to a country challenged by violence, poverty, & limited resources. This misguided decision also undermines stability in the region.\", \n",
    "                 \"The Colorado teacher walkouts are part of a growing movement around the country. Read more on why it's so important that we listen to our teachers:\",\n",
    "                \"By electing Jared Polis as our next governor, we'll be doing more than breaking another barrier; we'll be sending a fearless, progressive leader to the governor's office.\",\n",
    "                \"Climate change is real & the consequences are becoming a reality. If we want to preserve our CO way of life, and ensure our kids have clean air to breathe, we we canâ€™t afford to wait! As #COGov, I will bring CO to 100% renewable energy by 2040 (or sooner!)\"]\n",
    "\n",
    "senator = [\"Sen. Cory Gardner\", \"Sen. Cory Gardner\", \"Rep. Ken Buck\", \"Rep. Ken Buck\", \"Sen. Michael Bennet\", \"Sen. Michael Bennet\", \"Rep. Jared Polis\", \"Rep. Jared Polis\"]\n",
    "pred = poli_pipe.predict(congress_tweets)\n",
    "\n",
    "for i in range(len(congress_tweets)):\n",
    "    print(\"\\n{} tweeted: \\n{}\\n\\n Predicted message type: {}\\n----------------\".format(senator[i], congress_tweets[i], pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is there, and as discussed earlier, further tuning (beyond the scope of this project) would need occur. Political messages could be difficult to classify based on length of message, multiple topics of discussion, ambiguity in text, etc. The function provides a good baseline for future development and capabilities in terms of its ability to broadly classify messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conc'></a>\n",
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I was curious to see how well the NaiveBayes and Support Vector Machine classifiers performed for sentiment analysis, along with nltk (natural language toolkit). What I found was it did work rather well with any necessary text cleanup and minimizing text ambiguity. I think my understanding and interpretation of machine learning techniques improved while putting this together, but certainly completing similar would allow for a better grasp on the capabilities and limitations of implementing them in the future. All in all, text sentiment analysis proved to be a worthwhile challenge where I spent the majority of my time pre-processing the text data. \n",
    "\n",
    "* Future implementation (beyond scope of class) is using Wordnet, which is a powerful tool to find synonyms and antonyms. Possible use case would be translation of foreign words (would be interesting to see what the rest of the world thinks of U.S. presidents..cough..cough). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "## Reference: Common Stop Words\n",
    "{â€˜ourselvesâ€™, â€˜hersâ€™, â€˜betweenâ€™, â€˜yourselfâ€™, â€˜butâ€™, â€˜againâ€™, â€˜thereâ€™, â€˜aboutâ€™, â€˜onceâ€™, â€˜duringâ€™, â€˜outâ€™, â€˜veryâ€™, â€˜havingâ€™, â€˜withâ€™, â€˜theyâ€™, â€˜ownâ€™, â€˜anâ€™, â€˜beâ€™, â€˜someâ€™, â€˜forâ€™, â€˜doâ€™, â€˜itsâ€™, â€˜yoursâ€™, â€˜suchâ€™, â€˜intoâ€™, â€˜ofâ€™, â€˜mostâ€™, â€˜itselfâ€™, â€˜otherâ€™, â€˜offâ€™, â€˜isâ€™, â€˜sâ€™, â€˜amâ€™, â€˜orâ€™, â€˜whoâ€™, â€˜asâ€™, â€˜fromâ€™, â€˜himâ€™, â€˜eachâ€™, â€˜theâ€™, â€˜themselvesâ€™, â€˜untilâ€™, â€˜belowâ€™, â€˜areâ€™, â€˜weâ€™, â€˜theseâ€™, â€˜yourâ€™, â€˜hisâ€™, â€˜throughâ€™, â€˜donâ€™, â€˜norâ€™, â€˜meâ€™, â€˜wereâ€™, â€˜herâ€™, â€˜moreâ€™, â€˜himselfâ€™, â€˜thisâ€™, â€˜downâ€™, â€˜shouldâ€™, â€˜ourâ€™, â€˜theirâ€™, â€˜whileâ€™, â€˜aboveâ€™, â€˜bothâ€™, â€˜upâ€™, â€˜toâ€™, â€˜oursâ€™, â€˜hadâ€™, â€˜sheâ€™, â€˜allâ€™, â€˜noâ€™, â€˜whenâ€™, â€˜atâ€™, â€˜anyâ€™, â€˜beforeâ€™, â€˜themâ€™, â€˜sameâ€™, â€˜andâ€™, â€˜beenâ€™, â€˜haveâ€™, â€˜inâ€™, â€˜willâ€™, â€˜onâ€™, â€˜doesâ€™, â€˜yourselvesâ€™, â€˜thenâ€™, â€˜thatâ€™, â€˜becauseâ€™, â€˜whatâ€™, â€˜overâ€™, â€˜whyâ€™, â€˜soâ€™, â€˜canâ€™, â€˜didâ€™, â€˜notâ€™, â€˜nowâ€™, â€˜underâ€™, â€˜heâ€™, â€˜youâ€™, â€˜herselfâ€™, â€˜hasâ€™, â€˜justâ€™, â€˜whereâ€™, â€˜tooâ€™, â€˜onlyâ€™, â€˜myselfâ€™, â€˜whichâ€™, â€˜thoseâ€™, â€˜iâ€™, â€˜afterâ€™, â€˜fewâ€™, â€˜whomâ€™, â€˜tâ€™, â€˜beingâ€™, â€˜ifâ€™, â€˜theirsâ€™, â€˜myâ€™, â€˜againstâ€™, â€˜aâ€™, â€˜byâ€™, â€˜doingâ€™, â€˜itâ€™, â€˜howâ€™, â€˜furtherâ€™, â€˜wasâ€™, â€˜hereâ€™, â€˜thanâ€™}"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
