CSPB 3022 Classification Project - Political Text Analysis/Predictions Using Supervised Learning Methods
Theo Shin
ImageDrawing
Introduction
This dataset contains the classification of partisan bias, audience, and goals based on politicians' social media. Total rows total ~5000 and the objective is to clean, organize, manipulate data contents to derive meaningful conclusions based on hypotheses and analysis. For the final project, I have chosen sentiment analysis classifying political bias provided texts (in this case tweets). That is, to assign categories to the collection of data (political messaging) in order to aid in more accurate predictions and analysis. The idea of crowdsourcing draws on "wisdom of the crowd" arguments where I used the CrowdFlower/Kaggle database to access the data set to be used for this project, titled, "Political Social Media Posts" (https://www.kaggle.com/crowdflower/political-social-media-posts/data).

I chose this topic in particular, not because I'm fascinated with politics, but for the harnessing power of social media and was curious to see how natural language processing (NLP) could computationally identify and categorize opinions expressed in the political messages to determine potential biases.

Now that a multivariate analysis problem of interest has been identified, the outline of the project shall continue as follows:

Selecting Data Sources
Preprocess data; Cleaning and transforming data, as needed
Perform Exploratory Data Analysis (EDA)
Perform Classification
* References Disclaimer *
Websites used convert text into meaningful encoding vectors using some corpus (feature vectors may vary widely depending on the proximity of the corpus to the original problem), along with possibly tools to enhance visual flare.

If you referenced any web sites or solutions not of your own creation, list those references here:
Word Counter: https://data-flair.training/blogs/python-counter/
TextBlob Class: https://textblob.readthedocs.io/en/dev/api_reference.html
Classifiers: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html; https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/
TfidfTransformer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html
CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
Naive Bayes Classifier for Text Analysis: https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67
Pipeline: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
Sentiment Analysis Overview: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5635074/
Support Vector Machines: https://scikit-learn.org/stable/modules/svm.html
Installations:
textblob and wordcloud (pip install)
%matplotlib inline
import numpy as np
import scipy as sp
import scipy.stats as stats
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
from collections import Counter
from textblob import TextBlob
from textblob.classifiers import NaiveBayesClassifier
from textblob.classifiers import DecisionTreeClassifier
import scipy.stats
import sklearn.linear_model
import sklearn.discriminant_analysis
import sklearn.preprocessing
import sklearn.model_selection
import sklearn.neighbors
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split 
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import sklearn.linear_model as lm
from sklearn import metrics
from sklearn.pipeline import Pipeline
import nltk
from nltk.corpus import stopwords
from nltk.classify import SklearnClassifier
from wordcloud import WordCloud,STOPWORDS
import matplotlib.pyplot as plt
GitHub: https://github.com/tshin23/Political-Sentiment-Analysis
Table of Contents
Sentiment Analysis
Political Texts
Data Source
Dataset Description
Considerations
Exploratory Data Analysis
Expected Outcomes
Hypotheses
Quick Look at Data
Word Count
Character Count
# Hashtag Count
@ Directed Tweets
Average Word Length
Pre-Processing
Transform into Lower Case
Remove Punctuation
Stop Words
Word Frequency
Outlying Words
Spelling Bee
VISUALS for Text Analysis
Bar Graph for Related Words
Word Cloud
Spelling Bee
Classification
Multinomial Naive Bayes
Support Vector Machines
Predictive Analysis
Sentiment Text Analysis
Conclusion
Reference: Stop Words

Sentiment Analysis
Sentiment analysis is important in the studies of news values, public opinion, negative campaigning or political polarization and the expansion of digital textual data and efficient progress in automated text analysis provides vast opportunities for innovated social sciences [research] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5635074/). In general, a lack of tools and procedures for producing or collecting sentiment ratings of acceptable quality for large-scale data analyses currently limits the ease of use with this data mining technique. Typically with emphasis on large scale data, research projects can quickly overwhelm when faced with restrictions of time, money, limited trained coders.


Measuring Sentiment in Political Texts
Analyzing the polarity of texts has a long tradition in social sciences. A prominent example is media negativity, capturing the over-selection of negative over positive news, the tonality of media stories, and the degree of conflict or confrontation in news. With sentiment analysis, a large dataset can be classified with variying polarity, valence or tone and classified as simply positive, negative, or neutral. For the purposes of this project, supervised learning automated approaches will be employed to "learn" the sentiment of political messages (Note - the instructions governing the dataset will be kept at a beginner due to time constraints and limits on the tools available to me). Latter analyses will introduce supervised learning techniques from course discussions and set up the basics of supervised machine learning in the realm of text analysis.


Data Source
As noted in the introduction, the idea of crowdsourcing draws on "wisdom of the crowd" arguments where I used the CrowdFlower/Kaggle database to access the data set to be used for this project, titled, "Political Social Media Posts" (https://www.kaggle.com/crowdflower/political-social-media-posts/data). For project consideration, we will later delve deeper into selecting relevant words from the political messages to try and accurately predict meaning and predictions.

This dataset contains the results from contributors analyzing thousands of social media messages from U.S. Senators and other American politicians to classify its contents. Messages were broken down into audience (national or the tweeterâ€™s constituency), bias (neutral/bipartisan, or biased/partisan), and finally tagged as the actual substance of the message itself (options ranged from informational, announcement of a media appearance, an attack on another candidate, etc.)


Full Data Description: [political_social_media.csv]
unit_id : A unique id for the message
golden: always FALSE; (classifier for whether the message meets Crowdflower's gold standard)
unit_state: always "finalized"
trusted_judgments: the number of trusted human judgments that were entered for this message; an integer between 1 and 3
last_judgment_at: when the final judgment was collected
audience: one of national or constituency
audience:confidence: a measure of confidence in the audience judgment; a float between 0.5 and 1
bias: one of neutral of partisan
bias:confidence: a measure of confidence in the bias judgment; a float between 0.5 and 1
message: the aim of the message. one of: -- attack: the message attacks another politician
(0) constituency: the message discusses the politician's constituency
(1) information: an informational message about news in government or the wider U.S.
(2) media: a message about interaction with the media
(3) mobilization: a message intended to mobilize supporters
(4) other: a catch-all category for messages that don't fit into the other
(5) personal: a personal message, usually expressing sympathy, support or condolences, or other personal opinions
(6) policy: a message about political policy
(7) support: a message of political support
message:confidence: a measure of confidence in the message judgment; a float between 0.5 and 1
orig_golden: always empty; presumably whether some portion of the message was in the gold standard
audience_gold: always empty; presumably whether the audience response was in the gold standard
bias_gold: always empty; presumably whether the bias response was in the gold standard
bioid: a unique id for the politician
embed:HTML code to embed this message
id: unique id for the message WITHIN whichever social media site it was pulled from
label: a string of the form "From: firstname lastname (position from state)"
message_gold: always blank; presumably whether the message response was in the gold standard
source: where the message was posted; one of "facebook" or "twitter"
text: the text of the message

Dataset Considerations
Description from kaggle: This dataset, from Crowdflower's Data For Everyone Library, provides text of 5000 messages from politicians' social media accounts, along with human judgments about the purpose, partisanship, and audience of the messages. Because contents of this dataset are subject to human judgements, biases of the message components, it will be interesting to observe how much of the data is subjected to human persuasion (especially confidence level measurements).

Exploratory Data Analysis (EDA)

Expected Outcomes
From this point, I will:

Explore, clean, and modify the political message dataset to determine meaningful relationships between certain words and message types as indictaed by qualified human assessors from varying social media posts.

Determine how certain words in tweets/posts are correlated with political messages using heatmap/wordcloud.

Tranform data and perform linear, logistic regression, or other classification methods -- Decision tree classifiers: applying Naive Bayes or Support Vector Machine based algorithms for supervised machine-learning assessment.

Determine which classification based method is the stronger predictor for political bias/alignments between Multinomial Naive Bayes (MNB) text classification and Support Vector Machine (SVM) machine learning algorithms.


Hypotheses
The SVM classifier will out-predict and provide more accurate results than MNB

Anticipation of certain words being highly correlated with certain messages:

veterans = policy; support
bill = policy
In case you missed it (ICYMI) = media
abortion = policy

Quick Look at Data
Let's take a quick look at the first five rows

politext = pd.read_csv('political_social_media.csv')
politext.head()
Shape of dataset
print("There are {} observations and {} features in this dataset. \n".format(politext.shape[0],politext.shape[1]))
â€‹
#Looking deeper into the type of values in each column
for column in politext.columns:
    print(str.format("%s   \t# of unique values: %s,   \tdtype: %s" % (column, len(np.unique(politext[column].values)), politext[column].dtype)))

Let's count the number of words in each message
politext['word_count'] = politext['text'].apply(lambda x: len(str(x).split(" ")))
politext[['text','word_count']].head()

How about the number of characters?
politext['char_count'] = politext['text'].str.len() ## this also includes spaces
politext[['text','char_count']].head()

Number of hashtags per tweet
politext['hashtags'] = politext['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))
politext[['text','hashtags']].head()

Number of directed tweets (@)
politext['at'] = politext['text'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))
politext[['text','at']].head()

Average word length?
def avg_word(sentence):
  words = sentence.split()
  return (sum(len(word) for word in words)/len(words))
â€‹
politext['avg_word'] = politext['text'].apply(lambda x: avg_word(x))
politext[['text','avg_word']].head()

Data Pre-Processsing
Since the scope of the hypotheses are focused on message biases from the text, let's only focus on the relevant columns of "Message", "Bias", "Label", and "Text" and remove all other columns

# Remove columns to keep only those subject to analysis
politext = politext.drop(['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'audience', \
                'audience:confidence', 'bias:confidence', 'message:confidence', 'orig__golden', 'audience_gold', \
               'bias_gold', 'bioid', 'embed', 'id', 'message_gold', 'source'], axis=1)
â€‹
â€‹

Transform tweets into lower case
politext['text'] = politext['text'].apply(lambda x: " ".join(x.lower() for x in x.split()))
politext['text'].head()

Remove punctuation to clean data
politext['text'] = politext['text'].str.replace('[^\w\s]','')
politext['text'].head()

Removal of Stop Words
from nltk.corpus import stopwords
stop = stopwords.words('english')
politext['text'] = politext['text'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
politext['text'].head()

Word Frequency
freq = pd.Series(' '.join(politext['text']).split()).value_counts()[:10]
freq

Outliers: Rarest Words
freq = pd.Series(' '.join(politext['text']).split()).value_counts()[-10:]
freq
Let's Remove rarely occuring words (Removal reduces vocabulary clutter so features used later are effective)

# freq = list(freq.index)
# politext['text'] = politext['text'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
# politext['text'].head()

Spelling Mistakes/Further Message Pre-processing
from textblob import TextBlob
politext['text'][:5].apply(lambda x: str(TextBlob(x).correct()))
Prepare Bias and Message Columns to Categorical
# And Convert the bias and message columns to categorical (this will be helpful for classification methods later on)         
politext.bias = pd.Categorical(politext.bias)
politext.message = pd.Categorical(politext.message)
Create a "VECTOR" column to create a vector of the TEXT field
vector'
# Let's also set up a new column labeled "vector" for words in text field
politext['vector'] = [i for i in politext.text.str.split(" ")]
as' : 'BIAS', 'message' : 'MESSAGE', 'label' : 'LABEL', 'text' : 'TEXT', 'vector' : 'VE
# Let's now look at the first 10 rows (capitalized column headers)
politext.bias = pd.Categorical(politext.bias)
renameColumns = {'bias' : 'BIAS', 'message' : 'MESSAGE', 'label' : 'LABEL', 'text' : 'TEXT', 'vector' : 'VECTOR'}
politext.rename(mapper = renameColumns, axis = 1, inplace=True)
politext.head(10)
#Again let's look at the type of values for the remaining columns
for column in politext.columns:
    print("%s       \t# of unique values: %s,   \tdtype: %s" % (column, len(np.unique(politext[column].values)), politext[column].dtype))
print("There are {} observations and {} features in this dataset. \n".format(politext.shape[0],politext.shape[1]))
â€‹

VISUALS FOR TEXT ANALYSIS (EDA)
DETERMINING 50 MOST/LEAST COMMON WORDS USED IN TWEETS
count_words = Counter()
politext.VECTOR.apply(count_words.update)
print('MOST COMMON WORD (COUNT):\n', count_words.most_common(50))
print('\n\n')
print('LEAST COMMON WORD (COUNT):\n', count_words.most_common()[-250:-200])
Since we've already taken the liberty to clean up articles and common words (using stopwords) we're left with words that are relevantly least used along with words with incorrect spacing, continued misspellings, and hyperlinks. In this case, the least common words used may not have the intended effect for analysis.

For now, any such discrepancies will be noted and possibly mitigated depending on its effect on the data. In general, as noted during the initial proposal assessment, due to the sentiment analysis classifying political bias given text, categorical classification, correlation coefficient and further regression analysis need not to apply to describing the data. Instead, meaningful encoding vectors and classifiers (concepts discussed in the later weeks) will be used to make further sense of the data.


Bar Graph Representing Related Words
x = politext.MESSAGE.unique()
y = politext.MESSAGE.value_counts()
plt.bar(x, y);
plt.xticks(rotation=30);
plt.ylabel('Frequency');
print("Total Messages:\n",politext.MESSAGE.describe())
print("\nBiased word choices:\n",politext.BIAS.describe())
â€‹
â€‹
print("\nMessages containing the word 'proud':\n",politext.MESSAGE[politext.TEXT.str.contains("proud")].describe())
print("\nFor tweets containing the word 'love':\n",politext.MESSAGE[politext.TEXT.str.contains("love")].describe())
print("\nFor tweets containing the word 'hate':\n",politext.MESSAGE[politext.TEXT.str.contains("hate")].describe())
print("\nMessages containing the word 'ICYMI' (In Case You Missed It):\n",politext.MESSAGE[politext.TEXT.str.contains("icymi")].describe())
print("\nFor tweets containing the word 'Obama':\n",politext.MESSAGE[politext.TEXT.str.contains("obama")].describe())
print("\nFor tweets containing the word 'bill':\n",politext.MESSAGE[politext.TEXT.str.contains("bill")].describe())
print("\nFor tweets containing the word 'veterans':\n",politext.MESSAGE[politext.TEXT.str.contains("veterans")].describe())
print("\nFor tweets containing the word 'immigrants':\n",politext.MESSAGE[politext.TEXT.str.contains("immigrants")].describe())
print("\nFor tweets containing the word 'abortion':\n",politext.MESSAGE[politext.TEXT.str.contains("abortion")].describe())
The bar graph shows a heavy positive skew towards 'policy', 'attack', 'support' type messages. Though these results were somewhat expected, it may be a clear indicator of favoritism towards certain classes of words which influence the 2nd hypothesis (How certain words in tweets/posts are correlated with political messages and their implications).

data = politext['LABEL'].head(n=15000)
states = data.groupby(politext['LABEL']).count()
states.sort_values(ascending=False)
data = politext['LABEL'].head(n=15000)
states = data.groupby(politext['LABEL']).count()
states.sort_values(ascending=False)
â€‹

WORDCLOUD
#
#What it is..
#?WordCloud
Visual of WORDCLOUD
# Let's get a visual of the TEXT column
ptext = politext.TEXT[0]
â€‹
# Create and generate a world cloud image:
wordcloud = WordCloud().generate(ptext)
â€‹
# lower max_font_size, change maximum number of words:
wordcloud = WordCloud(max_font_size=50, max_words=100).generate(ptext)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
Delving deeper into word analysis
ptext = " ".join(review for review in politext.TEXT)
print ("There are {} words in the combination of all tweets.".format(len(ptext)))
'white'
#print(politext)
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords = stopwords.words('english')
# Create stopword list:
stopwords = set(STOPWORDS)
wordcloud = WordCloud(stopwords=stopwords, width=1600, height=800, background_color = 'white').generate(ptext)
plt.figure(figsize=(20,10), facecolor = 'k')
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.savefig('wordcloud.png', facecolor='k', bbox_inches='tight')
HYPOTHESIS #2: Anticipation of certain words being highly correlated with certain messages:
veterans = policy; support
bill = policy
In case you missed it (ICYMI) = policy (Original prediction was media)
Generally, many of the topics are correlated with policy and expressions such as "proud" and "love" are equated with personal-type messages, though "hate" came back as policy.


CLASSIFICATION
Preliminary Logistic Regression and Transforming the Text Data
Now for the fun part. For the data/text, the string values will be tokenized based upon the frequency of its occurrence in the corpus of the text. Based on the columns resulting from earlier data cleansing, the categorical values will be encoded into integers using a label encoder.

Following tokenization (process of replacing sensitive data with unique identification symbols that retain all the essential information about the data without compromising its security), the resulting data will be split into training and testing data. I'll begin by implementing a standard test size of 30% across each of the models to ascertain semi-optimal increase in the model's accuracy. Note - in cases of smaller test sizes, certain test cases may reveal higher accuracies due to coincidence.

In order for our classifiers to process the string data of our textual fields, they need to be tokenized to numerical vectors. This process is different for our target and main data. For the target, because it is a categorical variable, we need to encode those categories into integers using a Label Encoder. For our data, because it is a set of discrete strings, we need to tokenize them on the basis of the frequency of their occurance in the corpus of text.

After transforming our target and data, we need to split our training and testing data. We will use a standard test size of 33% of the dataset across our models, as this value seems to be semi-optimal for increasing our model's accuracy. With smaller test sizes, you may get higher point-accuracies for certain runs, purely due to coincidence.

## Split into train and test data
train, test = train_test_split(politext,test_size = 0.1)
# Removing " " for later use
train = train[train.VECTOR != ""]
# Label encoder definition
label_encode = sklearn.preprocessing.LabelEncoder()
# Data = social media text; Target = assigned message
import warnings; warnings.simplefilter('ignore')
X, y = politext.TEXT, politext.MESSAGE
label_encode.fit(y)
y = label_encode.transform(y)
â€‹
#tokenize the frequency of text occurrence and categorical values (target) to pass as numerals
vector_count = CountVectorizer()
X_counts = vector_count.fit_transform(X)
tfidf_X = TfidfTransformer() #transform count matrix to normalized tf(term frequency)-idf representation
token_X = tfidf_X.fit_transform(X_counts)
X = token_X
â€‹
#Splitting of data for training/testing
train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(X, y, test_size=0.3)
â€‹
#Label Encode messages
print("Categorical Targets:", np.unique(y))
print("Message Types:", np.unique(label_encode.inverse_transform(y)))

Applying Multinomial Naive Bayes (MNB) Classification for Text Analysis
From medium.com and Wikipedia, Naive Bayes is a family of algorithms based on applying Bayes theorem with a strong(naive) assumption, that every feature is independent of the others, in order to predict the category of a given sample. Naive Bayes will be used for text categorization, in this case, evaluating the text as belonging to one category or another with word frequencies as features. I will be classifying the Targets/Message Types from a model predictor in a supervised learning setting.

For more information regarding the use of MNB for encoding vectors, please see: https://en.wikipedia.org/wiki/Naive_Bayes_classifier
MNB
#MNB initialization
MNB = MultinomialNB()
#MNB fit
classifier = MNB.fit(train_X, train_y)
#MNB predictor
predicted_classes = classifier.predict(test_X)
â€‹
warnings.simplefilter('ignore')
print("MNB Classification Metrics: \n\n", metrics.classification_report(test_y, predicted_classes, target_names = np.unique(label_encode.inverse_transform(y))))
print("MNB Confusion Matrix: \n\n", metrics.confusion_matrix(test_y, predicted_classes))
print("Accuracy of MNB prediction: ", np.mean(predicted_classes == test_y))
Accuracy: Overall, how often is the classifier correct?

Precision: When it predicts yes, how often is it correct?

F Score: This is a weighted average of the true positive rate (recall) and precision.

Based on the MNB metrics and confusion matrix, it seems to be the case of over-prediction. With 1500 unique words in the dataset this may be the result of higher word frequencies, which was noted earlier for the histogram of words in each class. The confusion matrix shows a heavy right-sided skew which could mean the MNB is only calculating particular messages. Let's compare the results with the SVM.


Meaningful Vector Encoding (SVM)
#Pipeline Creation
X, y = politext.TEXT, politext.MESSAGE
train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(X, y, test_size=0.33)
poli_pipe = Pipeline([('vector_count', CountVectorizer()), ('tfidf_X', TfidfTransformer()), ('pipeline', SGDClassifier(loss='squared_loss', penalty='elasticnet', alpha=1e-3, 
                                                                                                       random_state=42, max_iter=10, tol=None, class_weight='balanced')),]);
#Classification Fit
poli_class = poli_pipe.fit(train_X, train_y);
# Prediction for test messages; Classification metrics/confusion matrix for SVM
prediction = poli_pipe.predict(test_X)
print("Accuracy of MNB predictor: ", np.mean(prediction == test_y))
warnings.simplefilter('ignore')
print("\n\nSVM Classification Metrics: \n\n", metrics.classification_report(test_y, prediction))
print("\n\nSVM Confusion Matrix: \n\n", metrics.confusion_matrix(test_y, prediction))
For both the MNB and SVM classifiers, the accuracies came back with better than chance results (1/9 = 0.1111). However, accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). After numerous testing, the MNB classifier achieved accuracy of around ~ 0.33 and the SVM at ~ 0.318. As noted earlier, although the MNB achieves a certain level of accuracy, it struggles with over prediction and therefore those tendencies are reflected in the results. At the end of the day, if asked to choose between the two model predictors, the SVM remains quite accurate while still considering a wider range of message types. When fed larger textual data, the SVM would learn and continue to improve as sample size increases. Both models reflected positive results and could be a case-by-case basis on which model is better suited for different situations. Let's now examine a function to get a better idea of how the models interpreted which words for each message type.

Average Accuracy for Model Analysis
5
runs = 50
accuracy = np.empty(runs)
for i in range(len(accuracy)):
    train_X, test_X, train_y, test_y = sklearn.model_selection.train_test_split(X, y, test_size = 0.3) #same percentage as earlier
    poli_pipe.fit(train_X, train_y);
    predict = poli_pipe.predict(test_X)
    accuracy[i] = np.mean(predict == test_y)
    
print('The accuracy (average) for {} runs is: {}'.format(runs, np.mean(accuracy)))
For 50 runs, the average accuracy hovers around 0.312. For the model to be better than chance, we have to achieve over 0.11 probability of making a correct prediction. Essentially being able to guess correctly 1 our of every 9 times.

For the Multinomial NB classifier, accuracy was pretty high at near 40% levels. This is certainly better than chance, though fairly inconsistent and as noted accuracy isn't the most reliable metric. One glaring issue was its tendency to predict ALL tweets were exclusively only two message types. Because the majority of tweets fall into just a few message categories, the classifer over-predicts for those categories and naturally, is more likely report higher accuracy.

Given that our average accuracy over many runs for the SVM classifier is ~ 0.32, the model still performs fairly well. With a larger data set and more diverse set of labelled examples (where each message type has >5000 samples), our SVM model should be able to continue to improve.

While our accuracy is not strictly greater than the accuracy achieved with Multinomial NB, we can still conclude that Hypothesis #1 is correct and that the SVM is a better predictor than Multinomial Naive Bayes. The reasoning lies in the confusion matrix. The SVM very clearly considers each message type as a possibility which yields more applicable and usable results.


A Little More Sentiment Analysis
politext['TEXT'][:5].apply(lambda x: TextBlob(x).sentiment)
The result is a tuple representing polarity and subjectivity of each tweet. Values closer to 1 mean a positive sentiment and values closer to -1 means negative sentiment. We can being using this feature to build our machine learning models.

politext['sentiment'] = politext['TEXT'].apply(lambda x: TextBlob(x).sentiment[0] )
politext[['TEXT','sentiment']].head()

Predictive Analysis
â€‹
def inform_feature(vector_count, pipeline, class_labels):
    names = vector_count.get_feature_names()
    for i, class_label in enumerate(class_labels):
        top = np.argsort(pipeline.coef_[i])[-20:]
        print("\n%s: %s\n" % (class_label, " ".join(names[j] for j in top)))
        
print("Featured Words Interpreted for Message Types:\n")
inform_feature(poli_pipe.get_params()['vector_count'], poli_pipe.get_params()['pipeline'], np.unique(y))
The inform_feature function prints the most informative words the models used to predict the message type for a given tweet. Glancing over the printed list, which is not static but rather changes each train/test split, many of these features make intuitive sense. For instance, in 'support' the expressions 'congratulations,' 'honoring,' and 'thank' are used. For 'attack' you see 'enforcing' and 'obamacare', in 'media' you see 'cspan', 'live', 'watch', 'interview', and 'tune'. You also see some clear examples of overfitting. If there is a single tweet with a unique word mapping to one message type, it is likely that the model will use it and place heavy emphasis on the unique feature. Certain words do map strongly towards specific message types and therefore, some tweaking would be involved (beyond the scope of this project, perhaps). The SVM model predicts fairly well and has an additional capabilities.

Capabilities (Beyond Scope of Project)
Looking at tweets from four Congressional representatives from Colorado, we can do a hypothetical test where potentially the capabilities of the predictor could be used, for example, to be linked with a live Twitter media feed, continuously consuming live content, and classfying the tweets. In turn, with more complex implemenation, the predictors could do more than just classify tweets and have a wide realm of use in various industries.

congress
congress_tweets = ["ICYMI: Be sure to watch this @FOX21News clip on my recent trip to Afghanistan to visit our Colorado-based troops. I'm incredibly proud of the work these men and women do everyday.", 
                 "My job is to represent Colorado in the United States Senate and that means working across party lines to get things done for the state. I'm proud to be ranked as the 8th most bipartisan Senator. More info here:",
                "Having only a few hours to read and digest huge bills is an absurd way to run a government. Youâ€™d be upset if your teacher assigned War and Peace in the morning and tested you on it later that afternoon. #READIT", 
                 "Please keep in your prayers all of those impacted by the fires burning across our district right now and those who are working to combat these blazes.",
                "Ending TPS for Hondurans forces tens of thousands of law-abiding individualsâ€”many of whom are parents to children who are U.S. citizensâ€”back to a country challenged by violence, poverty, & limited resources. This misguided decision also undermines stability in the region.", 
                 "The Colorado teacher walkouts are part of a growing movement around the country. Read more on why it's so important that we listen to our teachers:",
                "By electing Jared Polis as our next governor, we'll be doing more than breaking another barrier; we'll be sending a fearless, progressive leader to the governor's office.",
                "Climate change is real & the consequences are becoming a reality. If we want to preserve our CO way of life, and ensure our kids have clean air to breathe, we we canâ€™t afford to wait! As #COGov, I will bring CO to 100% renewable energy by 2040 (or sooner!)"]
â€‹
authors = ["Sen. Cory Gardner", "Sen. Cory Gardner", "Rep. Ken Buck", "Rep. Ken Buck", "Sen. Michael Bennet", "Sen. Michael Bennet", "Rep. Jared Polis", "Rep. Jared Polis"]
pred = poli_pipe.predict(congress_tweets)
â€‹
for i in range(len(congress_tweets)):
    print("\n{} tweeted: \n{}\n\n Predicted message type: {}\n----------------".format(authors[i], congress_tweets[i], pred[i]))
The implementation is there, and as discussed earlier, further tuning (beyond the scope of this project) would need occur. Political messages could be difficult to classify based on length of message, multiple topics of discussion, ambiguity in text, etc. The function provides a good baseline for future development and capabilities in terms of its ability to broadly classify messages.


CONCLUSION
For this project, I was curious to see how well the NaiveBayes and Support Vector Machine classifiers performed for sentiment analysis, along with nltk (natural language toolkit). What I found was it did work rather well with any necessary text cleanup and minimizing text ambiguity. I think my understanding and interpretation of machine learning techniques improved while putting this together, but certainly completing similar would allow for a better grasp on the capabilities and limitations of implementing them in the future. All in all, text sentiment analysis proved to be a worthwhile challenge where I spent the majority of my time pre-processing the text data.

Future implementation (beyond scope of class) is using Wordnet, which is a powerful tool to find synonyms and antonyms. Possible use case would be translation of foreign words (would be interesting to see what the rest of the world thinks of U.S. presidents..cough..cough).
words
<a id='stopwords'></a>
## Reference: Common Stop Words
{â€˜ourselvesâ€™, â€˜hersâ€™, â€˜betweenâ€™, â€˜yourselfâ€™, â€˜butâ€™, â€˜againâ€™, â€˜thereâ€™, â€˜aboutâ€™, â€˜onceâ€™, â€˜duringâ€™, â€˜outâ€™, â€˜veryâ€™, â€˜havingâ€™, â€˜withâ€™, â€˜theyâ€™, â€˜ownâ€™, â€˜anâ€™, â€˜beâ€™, â€˜someâ€™, â€˜forâ€™, â€˜doâ€™, â€˜itsâ€™, â€˜yoursâ€™, â€˜suchâ€™, â€˜intoâ€™, â€˜ofâ€™, â€˜mostâ€™, â€˜itselfâ€™, â€˜otherâ€™, â€˜offâ€™, â€˜isâ€™, â€˜sâ€™, â€˜amâ€™, â€˜orâ€™, â€˜whoâ€™, â€˜asâ€™, â€˜fromâ€™, â€˜himâ€™, â€˜eachâ€™, â€˜theâ€™, â€˜themselvesâ€™, â€˜untilâ€™, â€˜belowâ€™, â€˜areâ€™, â€˜weâ€™, â€˜theseâ€™, â€˜yourâ€™, â€˜hisâ€™, â€˜throughâ€™, â€˜donâ€™, â€˜norâ€™, â€˜meâ€™, â€˜wereâ€™, â€˜herâ€™, â€˜moreâ€™, â€˜himselfâ€™, â€˜thisâ€™, â€˜downâ€™, â€˜shouldâ€™, â€˜ourâ€™, â€˜theirâ€™, â€˜whileâ€™, â€˜aboveâ€™, â€˜bothâ€™, â€˜upâ€™, â€˜toâ€™, â€˜oursâ€™, â€˜hadâ€™, â€˜sheâ€™, â€˜allâ€™, â€˜noâ€™, â€˜whenâ€™, â€˜atâ€™, â€˜anyâ€™, â€˜beforeâ€™, â€˜themâ€™, â€˜sameâ€™, â€˜andâ€™, â€˜beenâ€™, â€˜haveâ€™, â€˜inâ€™, â€˜willâ€™, â€˜onâ€™, â€˜doesâ€™, â€˜yourselvesâ€™, â€˜thenâ€™, â€˜thatâ€™, â€˜becauseâ€™, â€˜whatâ€™, â€˜overâ€™, â€˜whyâ€™, â€˜soâ€™, â€˜canâ€™, â€˜didâ€™, â€˜notâ€™, â€˜nowâ€™, â€˜underâ€™, â€˜heâ€™, â€˜youâ€™, â€˜herselfâ€™, â€˜hasâ€™, â€˜justâ€™, â€˜whereâ€™, â€˜tooâ€™, â€˜onlyâ€™, â€˜myselfâ€™, â€˜whichâ€™, â€˜thoseâ€™, â€˜iâ€™, â€˜afterâ€™, â€˜fewâ€™, â€˜whomâ€™, â€˜tâ€™, â€˜beingâ€™, â€˜ifâ€™, â€˜theirsâ€™, â€˜myâ€™, â€˜againstâ€™, â€˜aâ€™, â€˜byâ€™, â€˜doingâ€™, â€˜itâ€™, â€˜howâ€™, â€˜furtherâ€™, â€˜wasâ€™, â€˜hereâ€™, â€˜thanâ€™}
